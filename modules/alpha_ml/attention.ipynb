{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.9967, -0.9330, -0.0326, -0.4899],\n",
      "         [ 0.4179,  0.4514,  0.0839,  3.0982],\n",
      "         [ 3.3712,  2.2387,  0.7966, -0.6468]],\n",
      "\n",
      "        [[-2.0937,  1.1803,  0.9583, -1.3858],\n",
      "         [-0.5227,  0.6232, -0.3494, -0.4605],\n",
      "         [-0.1747, -0.5465,  0.5057,  0.8110]]])\n",
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.randn(2,3,4) # batch_size, sequence_length, embedding_dim\n",
    "print(X)\n",
    "print(X.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0324, -0.1149, -0.0419, -0.1556, -0.2403],\n",
      "         [-0.0262, -0.0608, -0.0163, -0.1013, -0.1576],\n",
      "         [-0.0099,  0.0821, -0.0259, -0.1905,  0.0177]],\n",
      "\n",
      "        [[-0.0239,  0.0768,  0.0303, -0.0546,  0.0119],\n",
      "         [-0.0138,  0.0477,  0.0395, -0.0445, -0.0021],\n",
      "         [-0.0533,  0.0566,  0.2021, -0.0471, -0.0657]]],\n",
      "       grad_fn=<BmmBackward>)\n",
      "torch.Size([2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from function.modeling import AttentionSelf\n",
    "\n",
    "self_att = AttentionSelf(4, 100, 5) # embedding_dim, dim_k, dim_v\n",
    "res = self_att(X)\n",
    "print(res)\n",
    "print(res.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3305, -0.6529, -0.1530,  0.3678, -0.9866],\n",
      "         [-0.1778, -0.4165,  0.3536, -0.1222,  0.1489],\n",
      "         [ 0.4404, -0.2942, -0.0023,  0.2176, -0.3609]],\n",
      "\n",
      "        [[ 0.0463, -0.4390,  0.1155,  0.0371, -0.3586],\n",
      "         [ 0.0091, -0.1093,  0.3935,  0.3817,  0.2474],\n",
      "         [-0.0126,  0.1208,  0.4828,  0.5225,  0.4313]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([2, 3, 5])\n",
      "tensor([[[1.2051e-01, 8.7831e-01, 1.1845e-03],\n",
      "         [9.9998e-01, 1.9148e-05, 1.1398e-08],\n",
      "         [2.4400e-05, 8.9651e-20, 9.9998e-01]],\n",
      "\n",
      "        [[2.3499e-04, 4.2739e-02, 9.5703e-01],\n",
      "         [5.5140e-01, 3.8284e-01, 6.5763e-02],\n",
      "         [9.8708e-01, 7.0562e-03, 5.8604e-03]]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from function.modeling import AttentionMulitHead\n",
    "\n",
    "self_att = AttentionMulitHead(4, 100, 5) # embedding_dim, dim_k, dim_v\n",
    "res, score = self_att(X,X)\n",
    "print(res)\n",
    "print(res.size())\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d157ae46664ef61827425b1700af4733ccc165bd48e667b428e957754f1cdd7e"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('env_bot': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
