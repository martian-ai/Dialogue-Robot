{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Dialogue-Robot/resources/embedding/sentence-transformers/paraphrase-MiniLM-L6-v2/1_Pooling\\\\config.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 13\u001b[0m\n\u001b[0;32m      8\u001b[0m list_of_unpreprocessed_documents \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthis is the first line\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthis is the second line\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwell we have a lot of lines here more lines\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlike this it should not break\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexamples examples examples\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     10\u001b[0m list_of_preprocessed_documents \u001b[38;5;241m=\u001b[39m  [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthis is the first line\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthis is the second line\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwell we have a lot of lines here more lines\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlike this it should not break\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexamples examples examples\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 13\u001b[0m training_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mqt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_for_contextual\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlist_of_unpreprocessed_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_for_bow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlist_of_preprocessed_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m ctm \u001b[38;5;241m=\u001b[39m CombinedTM(bow_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(qt\u001b[38;5;241m.\u001b[39mvocab), contextual_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m768\u001b[39m, n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m) \u001b[38;5;66;03m# 50 topics\u001b[39;00m\n\u001b[0;32m     17\u001b[0m ctm\u001b[38;5;241m.\u001b[39mfit(training_dataset) \u001b[38;5;66;03m# run the model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\contextualized_topic_models-2.5.0-py3.9.egg\\contextualized_topic_models\\utils\\data_preparation.py:129\u001b[0m, in \u001b[0;36mTopicModelDataPreparation.fit\u001b[1;34m(self, text_for_contextual, text_for_bow, labels, custom_embeddings)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[39m# if the user is passing custom embeddings we don't need to create the embeddings using the model\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39mif\u001b[39;00m custom_embeddings \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 129\u001b[0m     train_contextualized_embeddings \u001b[39m=\u001b[39m bert_embeddings_from_list(\n\u001b[0;32m    130\u001b[0m         text_for_contextual,\n\u001b[0;32m    131\u001b[0m         sbert_model_to_load\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontextualized_model,\n\u001b[0;32m    132\u001b[0m         max_seq_length\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_seq_length,\n\u001b[0;32m    133\u001b[0m     )\n\u001b[0;32m    134\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    135\u001b[0m     train_contextualized_embeddings \u001b[39m=\u001b[39m custom_embeddings\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\contextualized_topic_models-2.5.0-py3.9.egg\\contextualized_topic_models\\utils\\data_preparation.py:50\u001b[0m, in \u001b[0;36mbert_embeddings_from_list\u001b[1;34m(texts, sbert_model_to_load, batch_size, max_seq_length)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbert_embeddings_from_list\u001b[39m(\n\u001b[0;32m     45\u001b[0m     texts, sbert_model_to_load, batch_size\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m, max_seq_length\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[0;32m     46\u001b[0m ):\n\u001b[0;32m     47\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[39m    Creates SBERT Embeddings from a list\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     model \u001b[39m=\u001b[39m SentenceTransformer(sbert_model_to_load)\n\u001b[0;32m     52\u001b[0m     \u001b[39mif\u001b[39;00m max_seq_length \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m         model\u001b[39m.\u001b[39mmax_seq_length \u001b[39m=\u001b[39m max_seq_length\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:95\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[1;34m(self, model_name_or_path, modules, device, cache_folder, use_auth_token)\u001b[0m\n\u001b[0;32m     87\u001b[0m         snapshot_download(model_name_or_path,\n\u001b[0;32m     88\u001b[0m                             cache_dir\u001b[39m=\u001b[39mcache_folder,\n\u001b[0;32m     89\u001b[0m                             library_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msentence-transformers\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     90\u001b[0m                             library_version\u001b[39m=\u001b[39m__version__,\n\u001b[0;32m     91\u001b[0m                             ignore_files\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mflax_model.msgpack\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrust_model.ot\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtf_model.h5\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     92\u001b[0m                             use_auth_token\u001b[39m=\u001b[39muse_auth_token)\n\u001b[0;32m     94\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(model_path, \u001b[39m'\u001b[39m\u001b[39mmodules.json\u001b[39m\u001b[39m'\u001b[39m)):    \u001b[39m#Load as SentenceTransformer model\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m     modules \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_sbert_model(model_path)\n\u001b[0;32m     96\u001b[0m \u001b[39melse\u001b[39;00m:   \u001b[39m#Load with AutoModel\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     modules \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_auto_model(model_path)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:840\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[1;34m(self, model_path)\u001b[0m\n\u001b[0;32m    838\u001b[0m \u001b[39mfor\u001b[39;00m module_config \u001b[39min\u001b[39;00m modules_config:\n\u001b[0;32m    839\u001b[0m     module_class \u001b[39m=\u001b[39m import_from_string(module_config[\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 840\u001b[0m     module \u001b[39m=\u001b[39m module_class\u001b[39m.\u001b[39;49mload(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(model_path, module_config[\u001b[39m'\u001b[39;49m\u001b[39mpath\u001b[39;49m\u001b[39m'\u001b[39;49m]))\n\u001b[0;32m    841\u001b[0m     modules[module_config[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m module\n\u001b[0;32m    843\u001b[0m \u001b[39mreturn\u001b[39;00m modules\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\sentence_transformers\\models\\Pooling.py:117\u001b[0m, in \u001b[0;36mPooling.load\u001b[1;34m(input_path)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    116\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(input_path):\n\u001b[1;32m--> 117\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(input_path, \u001b[39m'\u001b[39;49m\u001b[39mconfig.json\u001b[39;49m\u001b[39m'\u001b[39;49m)) \u001b[39mas\u001b[39;00m fIn:\n\u001b[0;32m    118\u001b[0m         config \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(fIn)\n\u001b[0;32m    120\u001b[0m     \u001b[39mreturn\u001b[39;00m Pooling(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Dialogue-Robot/resources/embedding/sentence-transformers/paraphrase-MiniLM-L6-v2/1_Pooling\\\\config.json'"
     ]
    }
   ],
   "source": [
    "from contextualized_topic_models.models.ctm import CombinedTM\n",
    "from contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation\n",
    "from contextualized_topic_models.utils.data_preparation import bert_embeddings_from_file\n",
    "\n",
    "# qt = TopicModelDataPreparation(\"all-mpnet-base-v2\")\n",
    "qt = TopicModelDataPreparation(\"../Dialogue-Robot/resources/embedding/sentence-transformers/paraphrase-MiniLM-L6-v2/\")\n",
    "\n",
    "list_of_unpreprocessed_documents = ['this is the first line', 'this is the second line', 'well we have a lot of lines here more lines', 'like this it should not break', 'examples examples examples']\n",
    "\n",
    "list_of_preprocessed_documents =  ['this is the first line', 'this is the second line', 'well we have a lot of lines here more lines', 'like this it should not break', 'examples examples examples']\n",
    "\n",
    "\n",
    "training_dataset = qt.fit(text_for_contextual=list_of_unpreprocessed_documents, text_for_bow=list_of_preprocessed_documents)\n",
    "\n",
    "ctm = CombinedTM(bow_size=len(qt.vocab), contextual_size=768, n_components=50) # 50 topics\n",
    "\n",
    "ctm.fit(training_dataset) # run the model\n",
    "\n",
    "ctm.get_topics(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
