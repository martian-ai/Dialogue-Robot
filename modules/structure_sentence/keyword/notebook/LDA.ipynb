{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import math\n",
    "import jieba\n",
    "import jieba.posseg as posseg\n",
    "from jieba import analyse\n",
    "from gensim import corpora, models\n",
    "import functools\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载整个文件下的数据\n",
    "def load_whole_dataSet(datafolder_path):\n",
    "    prepared_data =[] \n",
    "    files = os.listdir(datafolder_path)\n",
    "    for file in files:\n",
    "        if not os.path.isdir(datafolder_path + file): #判断是否是文件夹，不是文件夹才打开\n",
    "            for line in open(datafolder_path+\"/\"+file, 'r', encoding='utf-8'):\n",
    "                prepared_data.append(line)\n",
    "    return prepared_data\n",
    "# 对加载的数据预处理\n",
    "def pre_dataSet(prepared_data, pos=False):\n",
    "    doc_list = []\n",
    "    for line in prepared_data:\n",
    "        content = line.strip()\n",
    "        seg_list = seg_to_list(content, pos)\n",
    "        filetr_list = word_filter(seg_list, pos)\n",
    "        doc_list.append(filetr_list)\n",
    "    return doc_list\n",
    "\n",
    "# 停用词\n",
    "def get_stopword_list(stopword_path):\n",
    "    stopword_list = [stopword.replace('\\n', ' ') for stopword in open(stopword_path, encoding='gbk').readlines()]\n",
    "    return stopword_list\n",
    "\n",
    "# jieba分词\n",
    "def seg_to_list(sentence, pos=False):\n",
    "    if not pos:\n",
    "        seg_list = jieba.cut(sentence)\n",
    "    else:\n",
    "        seg_list = posseg.cut(sentence)\n",
    "    return seg_list\n",
    "\n",
    "# 去除干扰词\n",
    "def word_filter(seg_list, stopword_path, pos=False):\n",
    "    stopword_list = get_stopword_list(stopword_path)\n",
    "    filter_list = []\n",
    "    for seg in seg_list:\n",
    "        if not pos:\n",
    "            word = seg\n",
    "            flag = 'n'\n",
    "        else:\n",
    "            word = seg.word\n",
    "            flag = seg.flag\n",
    "        if not flag.startswith('n'):\n",
    "            continue\n",
    "        if word not in stopword_list and len(word)>1:\n",
    "            filter_list.append(word)\n",
    "    return filter_list\n",
    "\n",
    "# 排序函数， 用于topK关键字按值排序\n",
    "def cmp(e1, e2):\n",
    "    res = np.sign(e1[1] - e2[1])\n",
    "    if res != 0:\n",
    "        return res\n",
    "    else:\n",
    "        a = e1[0] + e2[0]\n",
    "        b = e2[0] + e1[0]\n",
    "        if a > b:\n",
    "            return 1\n",
    "        elif a == b:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "# 主题模型\n",
    "class TopicModel(object):\n",
    "    # 三个传入参数：处理后的数据集，关键词数量，具体模型（LSI、LDA），主题数量\n",
    "    def __init__(self, doc_list, keyword_num, model='LSI', num_topics=4):\n",
    "        # 使用gensim的接口，将文本转为向量化表示\n",
    "        # 先构建词空间\n",
    "        self.dictionary = corpora.Dictionary(doc_list)\n",
    "        # 使用BOW模型向量化\n",
    "        corpus = [self.dictionary.doc2bow(doc) for doc in doc_list]\n",
    "        # 对每个词，根据tf-idf进行加权，得到加权后的向量表示\n",
    "        self.tfidf_model = models.TfidfModel(corpus)\n",
    "        self.tfidf_corpus = self.tfidf_model[corpus]\n",
    " \n",
    "        self.keyword_num = keyword_num\n",
    "        self.num_topics = num_topics\n",
    "        # 选择加载胡模型\n",
    "        if model == 'LSI':\n",
    "            self.model = self.train_lsi()\n",
    "        else:\n",
    "            self.model = self.train_lda()\n",
    "        # 得到数据集的主题-词分布\n",
    "        word_dic = self.word_dictionary(doc_list)\n",
    "        self.wordtopic_dic = self.get_wordtopic(word_dic)\n",
    " \n",
    "    # 向量化\n",
    "    def doc2bowvec(self, word_list):\n",
    "        vec_list = [1 if word in word_list else 0 for word in self.dictionary]\n",
    "        print(\"vec_list\", vec_list)\n",
    "        return vec_list\n",
    " \n",
    "    # 词空间构建方法和向量化方法，在没有gensim接口时的一般处理方法\n",
    "    def word_dictionary(self, doc_list):\n",
    "        dictionary = []\n",
    "        # 2级变1级结构\n",
    "        for doc in doc_list:\n",
    "            # extend和append 方法有何异同 容易出错\n",
    "            dictionary.extend(doc)\n",
    " \n",
    "        dictionary = list(set(dictionary))\n",
    " \n",
    "        return dictionary\n",
    " \n",
    "    # 得到数据集的主题 - 词分布\n",
    "    def get_wordtopic(self, word_dic):\n",
    "        wordtopic_dic = {}\n",
    "        for word in word_dic:\n",
    "            singlist = [word]\n",
    "            # 计算每个词的加权向量\n",
    "            word_corpus = self.tfidf_model[self.dictionary.doc2bow(singlist)]\n",
    "            # 计算每个词的主题向量\n",
    "            word_topic = self.model[word_corpus]\n",
    "            wordtopic_dic[word] = word_topic\n",
    " \n",
    "        return wordtopic_dic\n",
    " \n",
    "    def train_lsi(self):\n",
    "        lsi = models.LsiModel(self.tfidf_corpus, id2word=self.dictionary, num_topics=self.num_topics)\n",
    "        return lsi\n",
    " \n",
    "    def train_lda(self):\n",
    "        lda = models.LdaModel(self.tfidf_corpus, id2word=self.dictionary, num_topics=self.num_topics)\n",
    "        return lda\n",
    " \n",
    "    # 计算词的分布和文档的分布的相似度，取相似度最高的keyword_num个词作为关键词\n",
    "    def get_simword(self, word_list):\n",
    "        # 文档的加权向量\n",
    "        sentcorpus = self.tfidf_model[self.dictionary.doc2bow(word_list)]\n",
    "        # 文档主题 向量\n",
    "        senttopic = self.model[sentcorpus]\n",
    "        # senttopic [(0, 0.03457821), (1, 0.034260772), (2, 0.8970413), (3, 0.034119748)]\n",
    "        # 余弦相似度计算\n",
    " \n",
    "        def calsim(l1, l2):\n",
    "            a, b, c = 0.0, 0.0, 0.0\n",
    "            for t1, t2 in zip(l1, l2):\n",
    "                x1 = t1[1]\n",
    "                x2 = t2[1]\n",
    "                a += x1 * x1\n",
    "                b += x1 * x1\n",
    "                c += x2 * x2\n",
    "            sim = a / math.sqrt(b * c) if not (b * c) == 0.0 else 0.0\n",
    "            return sim\n",
    "        # 计算输入文本和每个词的主题分布相似度\n",
    "        sim_dic = {}\n",
    "        for k, v in self.wordtopic_dic.items():\n",
    "            # 还是计算每个文档中的词和文档的相识度\n",
    "            if k not in word_list:\n",
    "                continue\n",
    "            sim = calsim(v, senttopic)\n",
    "            sim_dic[k] = sim\n",
    "            \n",
    "        counts = {}\n",
    "        keyWordDict = []\n",
    "        for k, v in sorted(sim_dic.items(), key=functools.cmp_to_key(cmp), reverse=True)[:self.keyword_num]:\n",
    "            if k is not None:\n",
    "                keyWordDict.append(k)\n",
    "        return keyWordDict\n",
    "    \n",
    "# 提取文件夹下所有文件的关键词，仅用到构建关键词字典\n",
    "def load_whole_dataSet(datafolder_path, pos=False, model='LDA', keyword_num=100):\n",
    "    prepared_data =[] \n",
    "    files = os.listdir(datafolder_path)\n",
    "    counts = {}\n",
    "    for file in tqdm(files):\n",
    "        prepared_data =[]\n",
    "        if not os.path.isdir(file): #判断是否是文件夹，不是文件夹才打开\n",
    "            for line in open(datafolder_path+\"/\"+file, 'r', encoding='utf-8'):\n",
    "                content = line.strip()\n",
    "                seg_list = seg_to_list(content, pos)\n",
    "                filetr_list = word_filter(seg_list, pos)\n",
    "                prepared_data.append(filetr_list)\n",
    "        if prepared_data:\n",
    "            try:\n",
    "                topic_model = TopicModel(prepared_data, keyword_num, model=model)\n",
    "            except ValueError as e:\n",
    "                pass\n",
    "        for doc_list_i in prepared_data:\n",
    "            keyWordDict = topic_model.get_simword(doc_list_i)\n",
    "            for word in keyWordDict:\n",
    "                if len(word) == 1:\n",
    "                    continue\n",
    "                else:\n",
    "                    counts[word] = counts.get(word, 0) + 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\t# 设置停用词路径\n",
    "    stopword_path = '~/dataSet/stop_words/stop_words.txt'\n",
    "    # ket_word_num 的数量大小在下一篇文章中给出\n",
    "    keyword_num = 10\n",
    "    # text为需要提取关键词的文本\n",
    "    text = \"^ ^\"\n",
    "    prepared_data = []\n",
    "    content = text.strip()\n",
    "    seg_list = seg_to_list(content, pos=False)\n",
    "    filetr_list = word_filter(seg_list, pos=False)\n",
    "    prepared_data.append(filetr_list)\n",
    "    topic_model = TopicModel(prepared_data, keyword_num, model='LDA')\n",
    "    counts = {}\n",
    "    for item in prepared_data:\n",
    "        keyWordDict = topic_model.get_simword(item)\n",
    "        for word in keyWordDict:\n",
    "            if len(word) == 1:\n",
    "                continue\n",
    "            else:\n",
    "                counts[word] = counts.get(word, 0) + 1\n",
    "    print(counts)"
   ]
  }
 ]
}