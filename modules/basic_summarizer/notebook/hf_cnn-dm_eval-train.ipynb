{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging face eval cnn-dm\n",
    "+ ä½¿ç”¨ cnn dailymail é€šè¿‡ Hugging Face çš„å„ä¸ªæ¨¡å‹æ•ˆæœè¯„ä¼°\n",
    "+ https://github.com/hellotransformers/Natural_Language_Processing_with_Transformers\n",
    "+ https://github.com/hellotransformers/Natural_Language_Processing_with_Transformers/blob/main/chapter6.md\n",
    "+ https://xiaosheng.run/2022/03/29/transformers-note-8.html\n",
    "+ https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A04-%E4%BD%BF%E7%94%A8Transformers%E8%A7%A3%E5%86%B3NLP%E4%BB%BB%E5%8A%A1/4.7-%E7%94%9F%E6%88%90%E4%BB%BB%E5%8A%A1-%E6%91%98%E8%A6%81%E7%94%9F%E6%88%90.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/users/sunhongchao/miniconda3/envs/bot-mvp-3/bin/python\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨ç»ˆç«¯å®‰è£…å®ŒåŒ…ä¹‹åè¦é‡å¯jupyter\n",
    "!which python\n",
    "# !pip list\n",
    "!export http_proxy=\"http://172.19.57.45:3128/\"\n",
    "!export https_proxy=\"http://172.19.57.45:3128/\"\n",
    "# !export TRANSFORMERS_OFFLINE=1 # å¼ºåˆ¶ç¦»çº¿åŠ è½½æ¨¡å‹\n",
    "# # !export TRANSFORMERS_OFFLINE=0 # å¼ºåˆ¶åœ¨çº¿\n",
    "# !export HF_DATASETS_OFFLINE=1 # å¼ºåˆ¶ç¦»çº¿åŠ è½½æ•°æ®\n",
    "# !export http_proxy=''\n",
    "# !export http_proxy=''\n",
    "# dataset 2.5.2\n",
    "# !pip install datasets==2.5.2\n",
    "# !pip install sacrebleu==2.3.1\n",
    "# rouge_score==0.0.4 work well\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æµ‹è¯•gpuä½¿ç”¨æƒ…å†µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/sunhongchao/miniconda3/envs/bot-mvp-3/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN DM æ•°æ®ä»‹ç»\n",
    "+ CNN/DailyMailæ•°æ®é›†ç”±å¤§çº¦300,000å¯¹æ–°é—»æ–‡ç« åŠå…¶ç›¸åº”çš„æ‘˜è¦ç»„æˆï¼Œè¿™äº›æ‘˜è¦ç”±CNNå’ŒDailyMailåœ¨å…¶æ–‡ç« ä¸­é™„åŠ çš„è¦ç‚¹ç»„æˆ\n",
    "+ è¯¥æ•°æ®é›†çš„ä¸€ä¸ªé‡è¦æ–¹é¢æ˜¯ï¼Œæ‘˜è¦æ˜¯æŠ½è±¡çš„ï¼Œè€Œä¸æ˜¯æ‘˜å½•çš„ï¼Œè¿™æ„å‘³ç€å®ƒä»¬ç”±æ–°çš„å¥å­è€Œä¸æ˜¯ç®€å•çš„æ‘˜å½•ç»„æˆ\n",
    "+ è¯¥æ•°æ®é›†å¯åœ¨Hubä¸Šæ‰¾åˆ°ï¼›æˆ‘ä»¬å°†ä½¿ç”¨3.0.0ç‰ˆæœ¬ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºæ‘˜è¦è€Œè®¾ç½®çš„éåŒ¿åç‰ˆæœ¬\n",
    "+ è®­ç»ƒé›†å¤§å°ï¼š 286817\n",
    "+ éªŒè¯é›†å¤§å°ï¼š 13368\n",
    "+ æµ‹è¯•é›†å¤§å°ï¼š 11487\n",
    "+ è®­ç»ƒé›†ä¸­å¹³å‡æ‘˜è¦å¥å­æ•°ï¼š 3.72"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ•°æ®å‡†å¤‡\n",
    "+ å› ä¸ºç½‘ç»œæˆ–è€…ä»£ç†çš„é—®é¢˜ï¼Œæ•°æ®ä»äº‘ç«¯ç›´æ¥ä¸‹è½½æœ‰é—®é¢˜ï¼Œè§£å†³æ–¹æ¡ˆå¦‚ä¸‹\n",
    "+ è¿œç¨‹åŠ è½½ å¯ä»¥å‚è€ƒ https://github.com/huggingface/datasets/issues/996\n",
    "+ æœ¬åœ°åŠ è½½ å¯ä»¥å‚è€ƒ https://blog.csdn.net/PolarisRisingWar/article/details/124042709"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['article', 'highlights', 'id']\n"
     ]
    }
   ],
   "source": [
    "# ä»£ç†å¿…é¡»å…³é—­\n",
    "# æœåŠ¡å™¨ä¸Šä¹Ÿéœ€è¦å…³é—­ä»£ç†\n",
    "# hide_output\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# è¿œç¨‹åŠ è½½\n",
    "# dataset = load_dataset(\"cnn_dailymail\",  version=\"3.0.0\") # æœ‰bug\n",
    "# dataset = load_dataset(\"ccdv/cnn_dailymail\",  version=\"3.0.0\")\n",
    "# æœ¬åœ°åŠ è½½\n",
    "dataset = datasets.load_from_disk('hf_cnn-dm')\n",
    "print(f\"Features: {dataset['train'].column_names}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¯¥æ•°æ®é›†æœ‰ä¸‰åˆ—ï¼šæ–‡ç« ï¼Œå…¶ä¸­åŒ…å«æ–°é—»æ–‡ç« ï¼Œäº®ç‚¹ä¸æ‘˜è¦ï¼Œä»¥åŠå”¯ä¸€æ ‡è¯†æ¯ç¯‡æ–‡ç« çš„ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Article (excerpt of 500 characters, total length: 3192):\n",
      "\n",
      "(CNN) -- Usain Bolt rounded off the world championships Sunday by claiming his third gold in Moscow as he anchored Jamaica to victory in the men's 4x100m relay. The fastest man in the world charged clear of United States rival Justin Gatlin as the Jamaican quartet of Nesta Carter, Kemar Bailey-Cole, Nickel Ashmeade and Bolt won in 37.36 seconds. The U.S finished second in 37.56 seconds with Canada taking the bronze after Britain were disqualified for a faulty handover. The 26-year-old Bolt has n\n",
      "\n",
      ">>> Summary (length: 180):\n",
      "Usain Bolt wins third gold of world championship .\n",
      "Anchors Jamaica to 4x100m relay victory .\n",
      "Eighth gold at the championships for Bolt .\n",
      "Jamaica double up in women's 4x100m relay .\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[\"train\"][1]\n",
    "print(f\"\"\"\n",
    ">>> Article (excerpt of 500 characters, total length: {len(sample[\"article\"])}):\n",
    "\"\"\")\n",
    "print(sample[\"article\"][:500])\n",
    "print(f'\\n>>> Summary (length: {len(sample[\"highlights\"])}):')\n",
    "print(sample[\"highlights\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CNN) -- Usain Bolt rounded off the world championships Sunday by claiming his third gold in Moscow as he anchored Jamaica to victory in the men's 4x100m relay. The fastest man in the world charged clear of United States rival Justin Gatlin as the Jamaican quartet of Nesta Carter, Kemar Bailey-Cole, Nickel Ashmeade and Bolt won in 37.36 seconds. The U.S finished second in 37.56 seconds with Canada taking the bronze after Britain were disqualified for a faulty handover. The 26-year-old Bolt has now collected eight gold medals at world championships, equaling the record held by American trio Carl Lewis, Michael Johnson and Allyson Felix, not to mention the small matter of six Olympic titles. The relay triumph followed individual successes in the 100 and 200 meters in the Russian capital. \"I'm proud of myself and I'll continue to work to dominate for as long as possible,\" Bolt said, having previously expressed his intention to carry on until the 2016 Rio Olympics. Victory was never seriously in doubt once he got the baton safely in hand from Ashmeade, while Gatlin and the United States third leg runner Rakieem Salaam had problems. Gatlin strayed out of his lane as he struggled to get full control of their baton and was never able to get on terms with Bolt. Earlier, Jamaica's women underlined their dominance in the sprint events by winning the 4x100m relay gold, anchored by Shelly-Ann Fraser-Pryce, who like Bolt was completing a triple. Their quartet recorded a championship record of 41.29 seconds, well clear of France, who crossed the line in second place in 42.73 seconds. Defending champions, the United States, were initially back in the bronze medal position after losing time on the second handover between Alexandria Anderson and English Gardner, but promoted to silver when France were subsequently disqualified for an illegal handover. The British quartet, who were initially fourth, were promoted to the bronze which eluded their men's team. Fraser-Pryce, like Bolt ag\n"
     ]
    }
   ],
   "source": [
    "sample_text = dataset[\"train\"][1][\"article\"][:2000]\n",
    "print(sample_text)\n",
    "# We'll collect the generated summaries of each model in a dictionary\n",
    "summaries = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from transformers import set_seed \n",
    "from transformers import pipeline\n",
    "import torch\n",
    "set_seed(42) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use bart in pytorch\n",
    "# from transformers import  pipeline\n",
    "# summarizer = pipeline(\"summarization\",proxy='')\n",
    "# summarizer(\"An apple a day, keeps the doctor away\", min_length=5, max_length=20)\n",
    "\n",
    "# # use t5 in tf\n",
    "# summarizer = pipeline(\"summarization\", model=\"t5-base\", tokenizer=\"t5-base\", framework=\"tf\")\n",
    "# summarizer(\"An apple a day, keeps the doctor away\", min_length=5, max_length=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_sentence_summary(text): \n",
    "\treturn \"\\n\".join(sent_tokenize(text)[:3]) \n",
    "summaries[\"baseline\"] = three_sentence_summary(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Olympic champion wins gold in Moscow.\n",
      "Bolt gets back in the picture with an impressive silver.\n",
      "A triple by Fraser-Pryce, Bailey-Cole, and Carter leaves the Jamaicans in the lead for the last part of the relay.\n",
      "Bolt dominates the final 50 meters and finishes off his hat trick and completes his double by completing the triple on the home straight, winning the gold with a world-\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"../../../resources/embedding/gpt2-xl\")\n",
    "# pipe = pipeline(\"text-generation\", model=\"gpt2-xl\")\n",
    "gpt2_query = sample_text + \"\\nTL;DR:\\n\" \n",
    "pipe_out = pipe(gpt2_query, max_length=512, clean_up_tokenization_spaces=True)\n",
    "\n",
    "summaries[\"gpt2\"] = \"\\n\".join( sent_tokenize(pipe_out[0][\"generated_text\"][len(gpt2_query) :]))\n",
    "print(summaries[\"gpt2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/sunhongchao/miniconda3/envs/bot-mvp-3/lib/python3.6/site-packages/transformers/generation_utils.py:745: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  beam_id = beam_token_id // vocab_size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usain bolt wins his third gold of the world championships in the men's 4x100m relay .\n",
      "the 26-year-old anchored the Jamaican quartet to victory in 37.36 seconds .\n",
      "he has now collected eight gold medals at the championships, equaling the record .\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"../../../resources/embedding/t5-large\") \n",
    "pipe_out = pipe(sample_text) \n",
    "summaries[\"t5\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))\n",
    "print(summaries[\"t5\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usain Bolt wins his third gold of the world championships in Moscow.\n",
      "Bolt anchors Jamaica to victory in the men's 4x100m relay.\n",
      "The 26-year-old has now won eight gold medals at world championships.\n",
      "Jamaica's women also win gold in the relay, beating France in the process.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"../../../resources/embedding/facebook_bart-large-cnn\") \n",
    "pipe_out = pipe(sample_text) \n",
    "summaries[\"bart\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))\n",
    "print(summaries[\"bart\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pegasus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usain Bolt wins third gold of world championships.\n",
      "Anchors Jamaica to victory in men's 4x100m relay.\n",
      "Eighth gold at the championships for Bolt.\n",
      "Jamaica also win women's 4x100m relay .\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    # pipe = pipeline(\"summarization\", model=\"../../../resources/embedding/google_pegasus-cnn-dailymail\") \n",
    "    pipe = pipeline(\"summarization\", model=\"google/pegasus-cnn_dailymail\") \n",
    "    pipe_out = pipe(sample_text) \n",
    "    summaries[\"pegasus\"] = pipe_out[0][\"summary_text\"].replace(\" .<n>\", \".\\n\")\n",
    "print(summaries[\"pegasus\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä¸åŒæ¨¡å‹æ•ˆæœå¯¹æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROUND TRUTH\n",
      "Usain Bolt wins third gold of world championship .\n",
      "Anchors Jamaica to 4x100m relay victory .\n",
      "Eighth gold at the championships for Bolt .\n",
      "Jamaica double up in women's 4x100m relay .\n",
      "\n",
      "BASELINE\n",
      "(CNN) -- Usain Bolt rounded off the world championships Sunday by claiming his third gold in Moscow as he anchored Jamaica to victory in the men's 4x100m relay.\n",
      "The fastest man in the world charged clear of United States rival Justin Gatlin as the Jamaican quartet of Nesta Carter, Kemar Bailey-Cole, Nickel Ashmeade and Bolt won in 37.36 seconds.\n",
      "The U.S finished second in 37.56 seconds with Canada taking the bronze after Britain were disqualified for a faulty handover.\n",
      "\n",
      "GPT2\n",
      "The Olympic champion wins gold in Moscow.\n",
      "Bolt gets back in the picture with an impressive silver.\n",
      "A triple by Fraser-Pryce, Bailey-Cole, and Carter leaves the Jamaicans in the lead for the last part of the relay.\n",
      "Bolt dominates the final 50 meters and finishes off his hat trick and completes his double by completing the triple on the home straight, winning the gold with a world-\n",
      "\n",
      "T5\n",
      "usain bolt wins his third gold of the world championships in the men's 4x100m relay .\n",
      "the 26-year-old anchored the Jamaican quartet to victory in 37.36 seconds .\n",
      "he has now collected eight gold medals at the championships, equaling the record .\n",
      "\n",
      "BART\n",
      "Usain Bolt wins his third gold of the world championships in Moscow.\n",
      "Bolt anchors Jamaica to victory in the men's 4x100m relay.\n",
      "The 26-year-old has now won eight gold medals at world championships.\n",
      "Jamaica's women also win gold in the relay, beating France in the process.\n",
      "\n",
      "PEGASUS\n",
      "Usain Bolt wins third gold of world championships.\n",
      "Anchors Jamaica to victory in men's 4x100m relay.\n",
      "Eighth gold at the championships for Bolt.\n",
      "Jamaica also win women's 4x100m relay .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"GROUND TRUTH\") \n",
    "print(dataset[\"train\"][1][\"highlights\"]) \n",
    "print(\"\") \n",
    "for model_name in summaries: \n",
    "\tprint(model_name.upper()) \n",
    "\tprint(summaries[model_name]) \n",
    "\tprint(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è¯„ä¼°æŒ‡æ ‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/users/sunhongchao/.cache/huggingface/modules/datasets_modules/metrics/sacrebleu/31e1673407d8789b8f5ddfd979948f6a1de0a6d691426d55fa74a35ffb0c1bdf (last modified on Tue Oct 25 00:02:35 2022) since it couldn't be found locally at sacrebleu, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric \n",
    "bleu_metric = load_metric(\"sacrebleu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>counts</th>\n",
       "      <td>[2, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>totals</th>\n",
       "      <td>[6, 5, 4, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precisions</th>\n",
       "      <td>[33.33, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bp</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sys_len</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ref_len</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Value\n",
       "score                            0\n",
       "counts                [2, 0, 0, 0]\n",
       "totals                [6, 5, 4, 3]\n",
       "precisions  [33.33, 0.0, 0.0, 0.0]\n",
       "bp                               1\n",
       "sys_len                          6\n",
       "ref_len                          6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "bleu_metric.add( prediction=\"the the the the the the\", reference=[\"the cat is on the mat\"]) \n",
    "results = bleu_metric.compute(smooth_method=\"floor\", smooth_value=0) \n",
    "results[\"precisions\"] = [np.round(p, 2) for p in results[\"precisions\"]] \n",
    "pd.DataFrame.from_dict(results, orient=\"index\", columns=[\"Value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>57.893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>counts</th>\n",
       "      <td>[5, 3, 2, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>totals</th>\n",
       "      <td>[5, 4, 3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precisions</th>\n",
       "      <td>[100.0, 75.0, 66.67, 50.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bp</th>\n",
       "      <td>0.818731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sys_len</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ref_len</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Value\n",
       "score                           57.893\n",
       "counts                    [5, 3, 2, 1]\n",
       "totals                    [5, 4, 3, 2]\n",
       "precisions  [100.0, 75.0, 66.67, 50.0]\n",
       "bp                            0.818731\n",
       "sys_len                              5\n",
       "ref_len                              6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_metric.add( prediction=\"the cat is on mat\", reference=[\"the cat is on the mat\"])\n",
    "results = bleu_metric.compute(smooth_method=\"floor\", smooth_value=0) \n",
    "results[\"precisions\"] = [np.round(p, 2)for p in results[\"precisions\"]] \n",
    "pd.DataFrame.from_dict(results, orient=\"index\", columns=[\"Value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/users/sunhongchao/.cache/huggingface/modules/datasets_modules/metrics/rouge/0ffdb60f436bdb8884d5e4d608d53dbe108e82dac4f494a66f80ef3f647c104f (last modified on Tue Oct 25 19:02:08 2022) since it couldn't be found locally at rouge, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "# rouge_score==0.0.4 work well\n",
    "rouge_metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.303571</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.232143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2</th>\n",
       "      <td>0.244898</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0.224490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5</th>\n",
       "      <td>0.472222</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.472222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bart</th>\n",
       "      <td>0.582278</td>\n",
       "      <td>0.207792</td>\n",
       "      <td>0.455696</td>\n",
       "      <td>0.506329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pegasus</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            rouge1    rouge2    rougeL  rougeLsum\n",
       "baseline  0.303571  0.090909  0.214286   0.232143\n",
       "gpt2      0.244898  0.000000  0.122449   0.224490\n",
       "t5        0.472222  0.228571  0.388889   0.472222\n",
       "bart      0.582278  0.207792  0.455696   0.506329\n",
       "pegasus   0.866667  0.655172  0.800000   0.833333"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference = dataset[\"train\"][1][\"highlights\"] \n",
    "records = [] \n",
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"] \n",
    "for model_name in summaries: \n",
    "\trouge_metric.add(prediction=summaries[model_name], reference=reference) \n",
    "\tscore = rouge_metric.compute() \n",
    "\trouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "\trecords.append(rouge_dict) \n",
    "pd.DataFrame.from_records(records, index=summaries.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨pegasus æŠ½æ ·è¯„ä¼° æµ‹è¯•é›†\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_summaries_baseline(dataset, metric, column_text=\"article\", column_summary=\"highlights\"): \n",
    "    summaries = [three_sentence_summary(text) for text in dataset[column_text]] \n",
    "    metric.add_batch(predictions=summaries, references=dataset[column_summary]) \n",
    "    score = metric.compute() \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at hf_cnn-dm/test/cache-f551f6b1b06308aa.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.388071</td>\n",
       "      <td>0.170554</td>\n",
       "      <td>0.247146</td>\n",
       "      <td>0.354972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            rouge1    rouge2    rougeL  rougeLsum\n",
       "baseline  0.388071  0.170554  0.247146   0.354972"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sampled = dataset[\"test\"].shuffle(seed=42).select(range(1000)) \n",
    "score = evaluate_summaries_baseline(test_sampled, rouge_metric) \n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names) \n",
    "pd.DataFrame.from_dict(rouge_dict, orient=\"index\", columns=[\"baseline\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "def chunks(list_of_elements, batch_size): \n",
    "\t\"\"\"Yield successive batch-sized chunks from list_of_elements.\"\"\" \n",
    "\tfor i in range(0, len(list_of_elements), batch_size): \n",
    "\t    yield list_of_elements[i : i + batch_size] \n",
    "def evaluate_summaries_pegasus(dataset, metric, model, tokenizer, batch_size=16, device=device, column_text=\"article\", column_summary=\"highlights\"): \n",
    "\tarticle_batches = list(chunks(dataset[column_text], batch_size)) \n",
    "\ttarget_batches = list(chunks(dataset[column_summary], batch_size)) \n",
    "\tfor article_batch, target_batch in tqdm( zip(article_batches, target_batches), total=len(article_batches)): \n",
    "\t\tinputs = tokenizer(article_batch, max_length=1024, truncation=True, padding=\"max_length\", return_tensors=\"pt\") \n",
    "\t\tsummaries = model.generate(input_ids=inputs[\"input_ids\"].to(device), attention_mask=inputs[\"attention_mask\"].to(device), length_penalty=0.8, num_beams=8, max_length=128) \n",
    "\t\tdecoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, clean_up_tokenization_spaces=True) for s in summaries] \n",
    "\t\tdecoded_summaries = [d.replace(\"<n>\", \" \") for d in decoded_summaries] \n",
    "\t\tmetric.add_batch(predictions=decoded_summaries, references=target_batch) \n",
    "\tscore = metric.compute() \n",
    "\treturn score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer \n",
    "\n",
    "# import torch\n",
    "# with torch.no_grad():\n",
    "#     model_ckpt = \"google/pegasus-cnn_dailymail\" \n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_ckpt) \n",
    "#     model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device) \n",
    "#     score = evaluate_summaries_pegasus(test_sampled, rouge_metric, model, tokenizer, batch_size=8) \n",
    "#     rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names) \n",
    "#     pd.DataFrame(rouge_dict, index=[\"pegasus\"])\n",
    "\n",
    "# print(rouge_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.38807104712508933, 'rouge2': 0.17055379457597958, 'rougeL': 0.2471461969561793, 'rougeLsum': 0.35497217427274075}\n"
     ]
    }
   ],
   "source": [
    "print(rouge_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è®­ç»ƒä¸€ä¸ªæ‘˜è¦æ¨¡å‹\n",
    "+ ä½¿ç”¨ SAMSum\n",
    "+ SAMSum æ•°æ®ä»‹ç»å¦‚ä¸‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split lengths: [14732, 819, 818]\n",
      "Features: ['id', 'dialogue', 'summary']\n",
      "\n",
      "Dialogue:\n",
      "Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Hannah: <file_gif>\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him ğŸ™‚\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye\n",
      "\n",
      "Summary:\n",
      "Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n"
     ]
    }
   ],
   "source": [
    "# dataset_samsum = load_dataset(\"samsum\") \n",
    "dataset_samsum = datasets.load_from_disk('hf_samsum')\n",
    "split_lengths = [len(dataset_samsum[split])for split in dataset_samsum] \n",
    "print(f\"Split lengths: {split_lengths}\") \n",
    "print(f\"Features: {dataset_samsum['train'].column_names}\") \n",
    "print(\"\\nDialogue:\") \n",
    "print(dataset_samsum[\"test\"][0][\"dialogue\"]) \n",
    "print(\"\\nSummary:\") \n",
    "print(dataset_samsum[\"test\"][0][\"summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æµæ°´çº¿è¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but you input_length is only 122. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "/home/users/sunhongchao/miniconda3/envs/bot-mvp-3/lib/python3.6/site-packages/transformers/generation_utils.py:745: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  beam_id = beam_token_id // vocab_size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Amanda: Ask Larry Amanda: He called her last time we were at the park together.\n",
      "Hannah: I'd rather you texted him.\n",
      "Amanda: Just text him .\n"
     ]
    }
   ],
   "source": [
    "pipe_out = pipe(dataset_samsum[\"test\"][0][\"dialogue\"]) \n",
    "print(\"Summary:\") \n",
    "print(pipe_out[0][\"summary_text\"].replace(\" .<n>\", \".\\n\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œè¯¥æ¨¡å‹å¤§å¤šè¯•å›¾é€šè¿‡æå–å¯¹è¯ä¸­çš„å…³é”®å¥å­æ¥è¿›è¡Œæ–‡æœ¬æ‘˜è¦ã€‚è¿™åœ¨CNN/DailyMailæ•°æ®é›†ä¸Šå¯èƒ½æ•ˆæœç›¸å¯¹è¾ƒå¥½ï¼Œä½†SAMSumä¸­çš„æ–‡æœ¬æ‘˜è¦æ›´åŠ æŠ½è±¡ã€‚è®©æˆ‘ä»¬é€šè¿‡åœ¨æµ‹è¯•é›†ä¸Šè¿è¡Œå®Œæ•´çš„ROUGEè¯„ä¼°æ¥ç¡®è®¤è¿™ä¸€ç‚¹:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-f4305cb53a2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_summaries_pegasus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_samsum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrouge_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dialogue\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_summary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"summary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrouge_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmeasure\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrouge_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrouge_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pegasus\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "score = evaluate_summaries_pegasus(dataset_samsum[\"test\"], rouge_metric, model, tokenizer, column_text=\"dialogue\", column_summary=\"summary\", batch_size=8) \n",
    "\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names) \n",
    "pd.DataFrame(rouge_dict, index=[\"pegasus\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç»“æœä¸æ˜¯å¾ˆå¥½ï¼Œä½†è¿™å¹¶ä¸æ„å¤–ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»è¿œç¦»äº†CNN/DailyMailçš„æ•°æ®åˆ†å¸ƒã€‚å°½ç®¡å¦‚æ­¤ï¼Œåœ¨è®­ç»ƒå‰è®¾ç½®è¯„ä¼°æµæ°´çº¿æœ‰ä¸¤ä¸ªå¥½å¤„ï¼šæˆ‘ä»¬å¯ä»¥ç›´æ¥ç”¨æŒ‡æ ‡æ¥è¡¡é‡è®­ç»ƒçš„æˆåŠŸä¸å¦ï¼Œè€Œä¸”æˆ‘ä»¬æœ‰ä¸€ä¸ªå¥½çš„åŸºçº¿ã€‚åœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸Šå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œåº”è¯¥ä¼šä½¿ROUGEæŒ‡æ ‡ç«‹å³å¾—åˆ°æ”¹å–„ï¼Œå¦‚æœä¸æ˜¯è¿™æ ·ï¼Œæˆ‘ä»¬å°±çŸ¥é“æˆ‘ä»¬çš„è®­ç»ƒå¾ªç¯å‡ºäº†é—®é¢˜ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å¾®è°ƒ pegasus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "d_len = [len(tokenizer.encode(s)) for s in dataset_samsum[\"train\"] [\"dialogue\"]] \n",
    "s_len = [len(tokenizer.encode(s)) for s in dataset_samsum[\"train\"][\"summary\"]] \n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3.5), sharey=True)\n",
    "axes[0].hist(d_len, bins=20, color=\"C0\", edgecolor=\"C0\") \n",
    "axes[0].set_title(\"Dialogue Token Length\") \n",
    "axes[0].set_xlabel(\"Length\") \n",
    "axes[0].set_ylabel(\"Count\") \n",
    "axes[1].hist(s_len, bins=20, color=\"C0\", edgecolor=\"C0\") \n",
    "axes[1].set_title(\"Summary Token Length\") \n",
    "axes[1].set_xlabel(\"Length\") \n",
    "plt.tight_layout() \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬çœ‹åˆ°ï¼Œå¤§å¤šæ•°å¯¹è¯æ¯”CNN/DailyMailçš„æ–‡ç« çŸ­å¾—å¤šï¼Œæ¯ä¸ªå¯¹è¯æœ‰100-200ä¸ªæ ‡è®°ã€‚åŒæ ·ï¼Œæ‘˜è¦ä¹ŸçŸ­å¾—å¤šï¼Œå¤§çº¦æœ‰20-40ä¸ªç¬¦å·ï¼ˆä¸€æ¡æ¨æ–‡çš„å¹³å‡é•¿åº¦ï¼‰\n",
    "è®©æˆ‘ä»¬åœ¨ä¸ºè®­ç»ƒè€…å»ºç«‹æ•°æ®æ•´ç†å™¨æ—¶ç‰¢è®°è¿™äº›æ„è§ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ•°æ®é›†è¿›è¡Œæ ‡è®°ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å°†å¯¹è¯å’Œæ‘˜è¦çš„æœ€å¤§é•¿åº¦åˆ†åˆ«è®¾ç½®ä¸º1024å’Œ128:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_examples_to_features(example_batch): \n",
    "# \tinput_encodings = tokenizer(example_batch[\"dialogue\"], max_length=1024, truncation=True) \n",
    "# \twith tokenizer.as_target_tokenizer(): \n",
    "# \t\ttarget_encodings = tokenizer(example_batch[\"summary\"], max_length=128, truncation=True) \n",
    "# \treturn {\"input_ids\": input_encodings[\"input_ids\"], \"attention_mask\": input_encodings[\"attention_mask\"], \"labels\": target_encodings[\"input_ids\"]} \n",
    "# dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched=True) \n",
    "# columns = [\"input_ids\", \"labels\", \"attention_mask\"] \n",
    "# dataset_samsum_pt.set_format(type=\"torch\", columns=columns)\n",
    "\n",
    "def convert_examples_to_features(example_batch): \n",
    "\tinput_encodings = tokenizer(example_batch[\"dialogue\"], max_length=1024, truncation=True) \n",
    "\t# with tokenizer.as_target_tokenizer(): \n",
    "\ttarget_encodings = tokenizer(example_batch[\"summary\"], max_length=128, truncation=True) \n",
    "\treturn {\"input_ids\": input_encodings[\"input_ids\"], \"attention_mask\": input_encodings[\"attention_mask\"], \"labels\": target_encodings[\"input_ids\"]} \n",
    "dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched=True) \n",
    "columns = [\"input_ids\", \"labels\", \"attention_mask\"] \n",
    "dataset_samsum_pt.set_format(type=\"torch\", columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½¿ç”¨æ ‡è®°åŒ–æ­¥éª¤çš„ä¸€ä¸ªæ–°ä¸œè¥¿æ˜¯tokenizer.as_target_tokenizer()ä¸Šä¸‹æ–‡ã€‚æœ‰äº›æ¨¡å‹åœ¨è§£ç å™¨è¾“å…¥ä¸­éœ€è¦ç‰¹æ®Šçš„æ ‡è®°ï¼Œæ‰€ä»¥åŒºåˆ†ç¼–ç å™¨å’Œè§£ç å™¨è¾“å…¥çš„æ ‡è®°å¾ˆé‡è¦ã€‚åœ¨withè¯­å¥ï¼ˆç§°ä¸ºä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼‰ä¸­ï¼Œæ ‡è®°å™¨çŸ¥é“å®ƒæ­£åœ¨ä¸ºè§£ç å™¨è¿›è¡Œæ ‡è®°ï¼Œå¹¶å¯ä»¥ç›¸åº”åœ°å¤„ç†åºåˆ—ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq \n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model) \n",
    "\n",
    "#ç„¶åï¼Œåƒå¾€å¸¸ä¸€æ ·ï¼Œæˆ‘ä»¬ä¸ºè®­ç»ƒè®¾ç½®äº†ä¸€ä¸ªTrainingArguments:\n",
    "\n",
    "from transformers import TrainingArguments, Trainer \n",
    "training_args = TrainingArguments( output_dir='pegasus-samsum', num_train_epochs=1, warmup_steps=500, per_device_train_batch_size=1, per_device_eval_batch_size=1, weight_decay=0.01, logging_steps=10, push_to_hub=True,\n",
    "evaluation_strategy='steps', eval_steps=500, save_steps=1e6, gradient_accumulation_steps=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, args=training_args, tokenizer=tokenizer, data_collator=seq2seq_data_collator, train_dataset=dataset_samsum_pt[\"train\"], eval_dataset=dataset_samsum_pt[\"validation\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train() \n",
    "score = evaluate_summaries_pegasus( dataset_samsum[\"test\"], rouge_metric, trainer.model, tokenizer, batch_size=2, column_text=\"dialogue\", column_summary=\"summary\") \n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names) \n",
    "pd.DataFrame(rouge_dict, index=[f\"pegasus\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('bot-mvp-3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7376174dba2c0f0f332eb5a05ceaaa2df22c27cd567ee777b9932514a8161813"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
