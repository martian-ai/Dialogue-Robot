{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging face eval cnn-dm\n",
    "+ 使用 cnn dailymail 通过 Hugging Face 的各个模型效果评估\n",
    "+ https://github.com/hellotransformers/Natural_Language_Processing_with_Transformers\n",
    "+ https://github.com/hellotransformers/Natural_Language_Processing_with_Transformers/blob/main/chapter6.md\n",
    "+ https://xiaosheng.run/2022/03/29/transformers-note-8.html\n",
    "+ https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A04-%E4%BD%BF%E7%94%A8Transformers%E8%A7%A3%E5%86%B3NLP%E4%BB%BB%E5%8A%A1/4.7-%E7%94%9F%E6%88%90%E4%BB%BB%E5%8A%A1-%E6%91%98%E8%A6%81%E7%94%9F%E6%88%90.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda info --envs\n",
    "# !conda init bash\n",
    "# !conda activate bot-mvp\n"
   ]
  },
  {
   "source": [
    "# CNN DM 数据介绍\n",
    "+ CNN/DailyMail数据集由大约300,000对新闻文章及其相应的摘要组成，这些摘要由CNN和DailyMail在其文章中附加的要点组成\n",
    "+ 该数据集的一个重要方面是，摘要是抽象的，而不是摘录的，这意味着它们由新的句子而不是简单的摘录组成\n",
    "+ 该数据集可在Hub上找到；我们将使用3.0.0版本，这是一个为摘要而设置的非匿名版本\n",
    "+ 训练集大小： 286817\n",
    "+ 验证集大小： 13368\n",
    "+ 测试集大小： 11487\n",
    "+ 训练集中平均摘要句子数： 3.72"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets==2.5.2\n",
    "# !pip uninstall transformers\n",
    "# !pip install transformers # 需要 3.1.0, 4.x 会报错\n",
    "# !export http_proxy='http://172.19.57.45:3128/'\n",
    "# !export http_proxy='http://172.19.57.45:3128/'\n",
    "# !export http_proxy=''\n",
    "# !export http_proxy=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python3\n",
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "source": [
    "# import pytorch_pretrained_bert as ppb\n",
    "# assert 'bert-large-cased' in ppb.modeling.PRETRAINED_MODEL_ARCHIVE_MAP"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip list"
   ]
  },
  {
   "source": [
    "# 数据准备\n",
    "+ 因为网络或者代理的问题，数据从云端直接下载有问题，解决方案如下\n",
    "+ 远程加载 可以参考 https://github.com/huggingface/datasets/issues/996\n",
    "+ 本地加载 可以参考 https://blog.csdn.net/PolarisRisingWar/article/details/124042709"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 代理必须关闭\n",
    "# 服务器上也需要关闭代理\n",
    "# hide_output\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 远程加载\n",
    "# dataset = load_dataset(\"cnn_dailymail\",  version=\"3.0.0\") # 有bug\n",
    "# dataset = load_dataset(\"ccdv/cnn_dailymail\",  version=\"3.0.0\")\n",
    "# 本地加载\n",
    "dataset = datasets.load_from_disk('hf_cnn-dm')\n"
   ]
  },
  {
   "source": [
    "该数据集有三列：文章，其中包含新闻文章，亮点与摘要，以及唯一标识每篇文章的ID"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Features: {dataset['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[\"train\"][1]\n",
    "print(f\"\"\"\n",
    "Article (excerpt of 500 characters, total length: {len(sample[\"article\"])}):\n",
    "\"\"\")\n",
    "print(sample[\"article\"][:500])\n",
    "print(f'\\nSummary (length: {len(sample[\"highlights\"])}):')\n",
    "print(sample[\"highlights\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = dataset[\"train\"][1][\"article\"][:2000]\n",
    "print(sample_text)\n",
    "# We'll collect the generated summaries of each model in a dictionary\n",
    "summaries = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use bart in pytorch\n",
    "# from transformers import  pipeline\n",
    "# summarizer = pipeline(\"summarization\")\n",
    "# summarizer(\"An apple a day, keeps the doctor away\", min_length=5, max_length=20)\n",
    "\n",
    "# # use t5 in tf\n",
    "# summarizer = pipeline(\"summarization\", model=\"t5-base\", tokenizer=\"t5-base\", framework=\"tf\")\n",
    "# summarizer(\"An apple a day, keeps the doctor away\", min_length=5, max_length=20)"
   ]
  },
  {
   "source": [
    "# baseline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_sentence_summary(text): \n",
    "\treturn \"\\n\".join(sent_tokenize(text)[:3]) \n",
    "summaries[\"baseline\"] = three_sentence_summary(sample_text)"
   ]
  },
  {
   "source": [
    "# gpt2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed \n",
    "from transformers import pipeline\n",
    "import torch\n",
    "set_seed(42) \n",
    "pipe = pipeline(\"text-generation\", model=\"gpt2-xl\", from_tf=True) \n",
    "gpt2_query = sample_text + \"\\nTL;DR:\\n\" \n",
    "pipe_out = pipe(gpt2_query, max_length=512, clean_up_tokenization_spaces=True)\n",
    "\n",
    "summaries[\"gpt2\"] = \"\\n\".join( sent_tokenize(pipe_out[0][\"generated_text\"][len(gpt2_query) :]))"
   ]
  },
  {
   "source": [
    "# t5"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"t5-large\") \n",
    "pipe_out = pipe(sample_text) \n",
    "summaries[\"t5\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))"
   ]
  },
  {
   "source": [
    "# bart"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\") \n",
    "pipe_out = pipe(sample_text) \n",
    "summaries[\"bart\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))"
   ]
  },
  {
   "source": [
    "# pegasus"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    pipe = pipeline(\"summarization\", model=\"google/pegasus-cnn_dailymail\") \n",
    "    pipe_out = pipe(sample_text) \n",
    "    summaries[\"pegasus\"] = pipe_out[0][\"summary_text\"].replace(\" .<n>\", \".\\n\")"
   ]
  },
  {
   "source": [
    "# 不同模型效果对比"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GROUND TRUTH\") \n",
    "print(dataset[\"train\"][1][\"highlights\"]) \n",
    "print(\"\") \n",
    "for model_name in summaries: \n",
    "\tprint(model_name.upper()) \n",
    "\tprint(summaries[model_name]) \n",
    "\tprint(\"\")"
   ]
  },
  {
   "source": [
    "# 评估指标"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#!pip install sacrebleu==2.3.1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric \n",
    "bleu_metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "bleu_metric.add( prediction=\"the the the the the the\", reference=[\"the cat is on the mat\"]) \n",
    "results = bleu_metric.compute(smooth_method=\"floor\", smooth_value=0) \n",
    "results[\"precisions\"] = [np.round(p, 2) for p in results[\"precisions\"]] \n",
    "pd.DataFrame.from_dict(results, orient=\"index\", columns=[\"Value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_metric.add( prediction=\"the cat is on mat\", reference=[\"the cat is on the mat\"])\n",
    "results = bleu_metric.compute(smooth_method=\"floor\", smooth_value=0) \n",
    "results[\"precisions\"] = [np.round(p, 2)for p in results[\"precisions\"]] \n",
    "pd.DataFrame.from_dict(results, orient=\"index\", columns=[\"Value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rouge_score==0.0.4 work well\n",
    "rouge_metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = dataset[\"train\"][1][\"highlights\"] \n",
    "records = [] \n",
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"] \n",
    "for model_name in summaries: \n",
    "\trouge_metric.add(prediction=summaries[model_name], reference=reference) \n",
    "\tscore = rouge_metric.compute() \n",
    "\trouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "\trecords.append(rouge_dict) \n",
    "pd.DataFrame.from_records(records, index=summaries.keys())"
   ]
  },
  {
   "source": [
    "# 使用pegsus 抽样评估 测试集\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_summaries_baseline(dataset, metric, column_text=\"article\", column_summary=\"highlights\"): \n",
    "    summaries = [three_sentence_summary(text) for text in dataset[column_text]] \n",
    "    metric.add_batch(predictions=summaries, references=dataset[column_summary]) \n",
    "    score = metric.compute() \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sampled = dataset[\"test\"].shuffle(seed=42).select(range(1000)) \n",
    "score = evaluate_summaries_baseline(test_sampled, rouge_metric) \n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names) \n",
    "pd.DataFrame.from_dict(rouge_dict, orient=\"index\", columns=[\"baseline\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "def chunks(list_of_elements, batch_size): \n",
    "\t\"\"\"Yield successive batch-sized chunks from list_of_elements.\"\"\" \n",
    "\tfor i in range(0, len(list_of_elements), batch_size): \n",
    "\t    yield list_of_elements[i : i + batch_size] \n",
    "def evaluate_summaries_pegasus(dataset, metric, model, tokenizer, batch_size=16, device=device, column_text=\"article\", column_summary=\"highlights\"): \n",
    "\tarticle_batches = list(chunks(dataset[column_text], batch_size)) \n",
    "\ttarget_batches = list(chunks(dataset[column_summary], batch_size)) \n",
    "\tfor article_batch, target_batch in tqdm( zip(article_batches, target_batches), total=len(article_batches)): \n",
    "\t\tinputs = tokenizer(article_batch, max_length=1024, truncation=True, padding=\"max_length\", return_tensors=\"pt\") \n",
    "\t\tsummaries = model.generate(input_ids=inputs[\"input_ids\"].to(device), attention_mask=inputs[\"attention_mask\"].to(device), length_penalty=0.8, num_beams=8, max_length=128) \n",
    "\t\tdecoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, clean_up_tokenization_spaces=True) for s in summaries] \n",
    "\t\tdecoded_summaries = [d.replace(\"<n>\", \" \") for d in decoded_summaries] \n",
    "\t\tmetric.add_batch(predictions=decoded_summaries, references=target_batch) \n",
    "\tscore = metric.compute() \n",
    "\treturn score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer \n",
    "\n",
    "# import torch\n",
    "# with torch.no_grad():\n",
    "#     model_ckpt = \"google/pegasus-cnn_dailymail\" \n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_ckpt) \n",
    "#     model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device) \n",
    "#     score = evaluate_summaries_pegasus(test_sampled, rouge_metric, model, tokenizer, batch_size=8) \n",
    "#     rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names) \n",
    "#     pd.DataFrame(rouge_dict, index=[\"pegasus\"])\n",
    "\n",
    "# print(rouge_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rouge_dict)"
   ]
  },
  {
   "source": [
    "# 训练一个摘要模型\n",
    "+ 使用 SAMSum\n",
    "+ SAMSum 数据介绍如下"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_samsum = load_dataset(\"samsum\") \n",
    "dataset_samsum = datasets.load_from_disk('hf_samsum')\n",
    "split_lengths = [len(dataset_samsum[split])for split in dataset_samsum] \n",
    "print(f\"Split lengths: {split_lengths}\") \n",
    "print(f\"Features: {dataset_samsum['train'].column_names}\") \n",
    "print(\"\\nDialogue:\") \n",
    "print(dataset_samsum[\"test\"][0][\"dialogue\"]) \n",
    "print(\"\\nSummary:\") \n",
    "print(dataset_samsum[\"test\"][0][\"summary\"])"
   ]
  },
  {
   "source": [
    "# 流水线评估"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_out = pipe(dataset_samsum[\"test\"][0][\"dialogue\"]) \n",
    "print(\"Summary:\") \n",
    "print(pipe_out[0][\"summary_text\"].replace(\" .<n>\", \".\\n\")) "
   ]
  },
  {
   "source": [
    "我们可以看到，该模型大多试图通过提取对话中的关键句子来进行文本摘要。这在CNN/DailyMail数据集上可能效果相对较好，但SAMSum中的文本摘要更加抽象。让我们通过在测试集上运行完整的ROUGE评估来确认这一点:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = evaluate_summaries_pegasus(dataset_samsum[\"test\"], rouge_metric, model, tokenizer, column_text=\"dialogue\", column_summary=\"summary\", batch_size=8) \n",
    "\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names) \n",
    "pd.DataFrame(rouge_dict, index=[\"pegasus\"])"
   ]
  },
  {
   "source": [
    "结果不是很好，但这并不意外，因为我们已经远离了CNN/DailyMail的数据分布。尽管如此，在训练前设置评估流水线有两个好处：我们可以直接用指标来衡量训练的成功与否，而且我们有一个好的基线。在我们的数据集上对模型进行微调，应该会使ROUGE指标立即得到改善，如果不是这样，我们就知道我们的训练循环出了问题。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 微调 pegsus\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "d_len = [len(tokenizer.encode(s)) for s in dataset_samsum[\"train\"] [\"dialogue\"]] \n",
    "s_len = [len(tokenizer.encode(s)) for s in dataset_samsum[\"train\"][\"summary\"]] \n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3.5), sharey=True)\n",
    "axes[0].hist(d_len, bins=20, color=\"C0\", edgecolor=\"C0\") \n",
    "axes[0].set_title(\"Dialogue Token Length\") \n",
    "axes[0].set_xlabel(\"Length\") \n",
    "axes[0].set_ylabel(\"Count\") \n",
    "axes[1].hist(s_len, bins=20, color=\"C0\", edgecolor=\"C0\") \n",
    "axes[1].set_title(\"Summary Token Length\") \n",
    "axes[1].set_xlabel(\"Length\") \n",
    "plt.tight_layout() \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "source": [
    "我们看到，大多数对话比CNN/DailyMail的文章短得多，每个对话有100-200个标记。同样，摘要也短得多，大约有20-40个符号（一条推文的平均长度）\n",
    "让我们在为训练者建立数据整理器时牢记这些意见。首先，我们需要对数据集进行标记。现在，我们将对话和摘要的最大长度分别设置为1024和128:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_examples_to_features(example_batch): \n",
    "# \tinput_encodings = tokenizer(example_batch[\"dialogue\"], max_length=1024, truncation=True) \n",
    "# \twith tokenizer.as_target_tokenizer(): \n",
    "# \t\ttarget_encodings = tokenizer(example_batch[\"summary\"], max_length=128, truncation=True) \n",
    "# \treturn {\"input_ids\": input_encodings[\"input_ids\"], \"attention_mask\": input_encodings[\"attention_mask\"], \"labels\": target_encodings[\"input_ids\"]} \n",
    "# dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched=True) \n",
    "# columns = [\"input_ids\", \"labels\", \"attention_mask\"] \n",
    "# dataset_samsum_pt.set_format(type=\"torch\", columns=columns)\n",
    "\n",
    "def convert_examples_to_features(example_batch): \n",
    "\tinput_encodings = tokenizer(example_batch[\"dialogue\"], max_length=1024, truncation=True) \n",
    "\t# with tokenizer.as_target_tokenizer(): \n",
    "\ttarget_encodings = tokenizer(example_batch[\"summary\"], max_length=128, truncation=True) \n",
    "\treturn {\"input_ids\": input_encodings[\"input_ids\"], \"attention_mask\": input_encodings[\"attention_mask\"], \"labels\": target_encodings[\"input_ids\"]} \n",
    "dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched=True) \n",
    "columns = [\"input_ids\", \"labels\", \"attention_mask\"] \n",
    "dataset_samsum_pt.set_format(type=\"torch\", columns=columns)"
   ]
  },
  {
   "source": [
    "使用标记化步骤的一个新东西是tokenizer.as_target_tokenizer()上下文。有些模型在解码器输入中需要特殊的标记，所以区分编码器和解码器输入的标记很重要。在with语句（称为上下文管理器）中，标记器知道它正在为解码器进行标记，并可以相应地处理序列。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq \n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model) \n",
    "\n",
    "#然后，像往常一样，我们为训练设置了一个TrainingArguments:\n",
    "\n",
    "from transformers import TrainingArguments, Trainer \n",
    "training_args = TrainingArguments( output_dir='pegasus-samsum', num_train_epochs=1, warmup_steps=500, per_device_train_batch_size=1, per_device_eval_batch_size=1, weight_decay=0.01, logging_steps=10, push_to_hub=True,\n",
    "evaluation_strategy='steps', eval_steps=500, save_steps=1e6, gradient_accumulation_steps=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, args=training_args, tokenizer=tokenizer, data_collator=seq2seq_data_collator, train_dataset=dataset_samsum_pt[\"train\"], eval_dataset=dataset_samsum_pt[\"validation\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train() \n",
    "score = evaluate_summaries_pegasus( dataset_samsum[\"test\"], rouge_metric, trainer.model, tokenizer, batch_size=2, column_text=\"dialogue\", column_summary=\"summary\") \n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names) pd.DataFrame(rouge_dict, index=[f\"pegasus\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13-final"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}