{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/sunhongchao/miniconda3/envs/bot-mvp/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import datasets\n",
    "from transformers import pipeline\n",
    "\n",
    "dataset_samsum = datasets.load_from_disk('../../../resources/dataset/hf_samsum/')\n",
    "pipe = pipeline(\"summarization\", model=\"../../../resources/embedding/google_pegasus-cnn-dm\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œè¯¥æ¨¡å‹å¤§å¤šè¯•å›¾é€šè¿‡æå–å¯¹è¯ä¸­çš„å…³é”®å¥å­æ¥è¿›è¡Œæ–‡æœ¬æ‘˜è¦ã€‚è¿™åœ¨CNN/DailyMailæ•°æ®é›†ä¸Šå¯èƒ½æ•ˆæœç›¸å¯¹è¾ƒå¥½ï¼Œä½†SAMSumä¸­çš„æ–‡æœ¬æ‘˜è¦æ›´åŠ æŠ½è±¡ã€‚è®©æˆ‘ä»¬é€šè¿‡åœ¨æµ‹è¯•é›†ä¸Šè¿è¡Œå®Œæ•´çš„ROUGEè¯„ä¼°æ¥ç¡®è®¤è¿™ä¸€ç‚¹:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but you input_length is only 122. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue:\n",
      "Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Hannah: <file_gif>\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him ğŸ™‚\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye\n",
      "Summary:\n",
      "Amanda: Ask Larry Amanda: He called her last time we were at the park together.\n",
      "Hannah: I'd rather you texted him.\n",
      "Amanda: Just text him .\n"
     ]
    }
   ],
   "source": [
    "pipe_out = pipe(dataset_samsum[\"test\"][0][\"dialogue\"]) \n",
    "print(\"Dialogue:\")\n",
    "print(dataset_samsum[\"test\"][0][\"dialogue\"])\n",
    "print(\"Summary:\") \n",
    "print(pipe_out[0][\"summary_text\"].replace(\" .<n>\", \".\\n\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/users/sunhongchao/.cache/huggingface/modules/datasets_modules/metrics/sacrebleu/31e1673407d8789b8f5ddfd979948f6a1de0a6d691426d55fa74a35ffb0c1bdf (last modified on Sat Oct 29 23:06:04 2022) since it couldn't be found locally at sacrebleu, or remotely on the Hugging Face Hub.\n",
      "Using the latest cached version of the module from /home/users/sunhongchao/.cache/huggingface/modules/datasets_modules/metrics/rouge/0ffdb60f436bdb8884d5e4d608d53dbe108e82dac4f494a66f80ef3f647c104f (last modified on Sat Oct 29 23:10:57 2022) since it couldn't be found locally at rouge, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric \n",
    "import pandas as pd \n",
    "bleu_metric = load_metric(\"sacrebleu\")\n",
    "rouge_metric = load_metric(\"rouge\") # rouge_score==0.0.4 work well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at ../../../resources/dataset/hf_samsum/test/cache-52b2c51951d462ff.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '13862856', 'dialogue': \"Hannah: Hey, do you have Betty's number?\\nAmanda: Lemme check\\nHannah: <file_gif>\\nAmanda: Sorry, can't find it.\\nAmanda: Ask Larry\\nAmanda: He called her last time we were at the park together\\nHannah: I don't know him well\\nHannah: <file_gif>\\nAmanda: Don't be shy, he's very nice\\nHannah: If you say so..\\nHannah: I'd rather you texted him\\nAmanda: Just text him ğŸ™‚\\nHannah: Urgh.. Alright\\nHannah: Bye\\nAmanda: Bye bye\", 'summary': \"Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\"}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_samsum[\"test\"][0])\n",
    "test_sampled = dataset_samsum[\"test\"].shuffle(seed=42).select(range(10)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\" \n",
    "\n",
    "print(device)\n",
    "\n",
    "def chunks(list_of_elements, batch_size): \n",
    "\t\"\"\"Yield successive batch-sized chunks from list_of_elements.\"\"\" \n",
    "\tfor i in range(0, len(list_of_elements), batch_size): \n",
    "\t    yield list_of_elements[i : i + batch_size] \n",
    "def evaluate_summaries_pegasus(dataset, metric, model, tokenizer, batch_size=16, device=device, column_text=\"dialogue\", column_summary=\"summary\"): \n",
    "\tarticle_batches = list(chunks(dataset[column_text], batch_size)) \n",
    "\ttarget_batches = list(chunks(dataset[column_summary], batch_size)) \n",
    "\tfor article_batch, target_batch in tqdm( zip(article_batches, target_batches), total=len(article_batches)): \n",
    "\t\tinputs = tokenizer(article_batch, max_length=1024, truncation=True, padding=\"max_length\", return_tensors=\"pt\") \n",
    "\t\tsummaries = model.generate(input_ids=inputs[\"input_ids\"].to(device), attention_mask=inputs[\"attention_mask\"].to(device), length_penalty=0.8, num_beams=8, max_length=128) \n",
    "\t\tdecoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, clean_up_tokenization_spaces=True) for s in summaries] \n",
    "\t\tdecoded_summaries = [d.replace(\"<n>\", \" \") for d in decoded_summaries] \n",
    "\t\tmetric.add_batch(predictions=decoded_summaries, references=target_batch) \n",
    "\tscore = metric.compute() \n",
    "\treturn score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:09<00:00,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.27189863876138387, 'rouge2': 0.05117642754979565, 'rougeL': 0.1987865541474696, 'rougeLsum': 0.197517473378596}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer \n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_ckpt = \"../../../resources/embedding/google_pegasus-cnn-dm\" \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device) \n",
    "    score = evaluate_summaries_pegasus(test_sampled, rouge_metric, model, tokenizer, batch_size=2) \n",
    "    rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names) \n",
    "    pd.DataFrame(rouge_dict, index=[\"pegasus\"])\n",
    "\n",
    "print(rouge_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ ç»“æœä¸æ˜¯å¾ˆå¥½ï¼Œä½†è¿™å¹¶ä¸æ„å¤–ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»è¿œç¦»äº†CNN/DailyMailçš„æ•°æ®åˆ†å¸ƒã€‚å°½ç®¡å¦‚æ­¤ï¼Œåœ¨è®­ç»ƒå‰è®¾ç½®è¯„ä¼°æµæ°´çº¿æœ‰ä¸¤ä¸ªå¥½å¤„ï¼šæˆ‘ä»¬å¯ä»¥ç›´æ¥ç”¨æŒ‡æ ‡æ¥è¡¡é‡è®­ç»ƒçš„æˆåŠŸä¸å¦ï¼Œè€Œä¸”æˆ‘ä»¬æœ‰ä¸€ä¸ªå¥½çš„åŸºçº¿ã€‚åœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸Šå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œåº”è¯¥ä¼šä½¿ROUGEæŒ‡æ ‡ç«‹å³å¾—åˆ°æ”¹å–„ï¼Œå¦‚æœä¸æ˜¯è¿™æ ·ï¼Œæˆ‘ä»¬å°±çŸ¥é“æˆ‘ä»¬çš„è®­ç»ƒå¾ªç¯å‡ºäº†é—®é¢˜ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
