{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# samsum æ•°æ®å¯¼å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apollo/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Directory ../../../resources/dataset/hf_samsum/ not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdatasets\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[0;32m----> 5\u001b[0m dataset_samsum \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39;49mload_from_disk(\u001b[39m'\u001b[39;49m\u001b[39m../../../resources/dataset/hf_samsum/\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/datasets/load.py:1880\u001b[0m, in \u001b[0;36mload_from_disk\u001b[0;34m(dataset_path, fs, keep_in_memory, storage_options)\u001b[0m\n\u001b[1;32m   1877\u001b[0m     path_join \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin\n\u001b[1;32m   1879\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fs\u001b[39m.\u001b[39mexists(dest_dataset_path):\n\u001b[0;32m-> 1880\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDirectory \u001b[39m\u001b[39m{\u001b[39;00mdataset_path\u001b[39m}\u001b[39;00m\u001b[39m not found\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1881\u001b[0m \u001b[39mif\u001b[39;00m fs\u001b[39m.\u001b[39misfile(path_join(dest_dataset_path, config\u001b[39m.\u001b[39mDATASET_INFO_FILENAME)) \u001b[39mand\u001b[39;00m fs\u001b[39m.\u001b[39misfile(\n\u001b[1;32m   1882\u001b[0m     path_join(dest_dataset_path, config\u001b[39m.\u001b[39mDATASET_STATE_JSON_FILENAME)\n\u001b[1;32m   1883\u001b[0m ):\n\u001b[1;32m   1884\u001b[0m     \u001b[39mreturn\u001b[39;00m Dataset\u001b[39m.\u001b[39mload_from_disk(dataset_path, keep_in_memory\u001b[39m=\u001b[39mkeep_in_memory, storage_options\u001b[39m=\u001b[39mstorage_options)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Directory ../../../resources/dataset/hf_samsum/ not found"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import datasets\n",
    "from transformers import pipeline\n",
    "\n",
    "dataset_samsum = datasets.load_from_disk('../../../resources/dataset/hf_samsum/')\n",
    "# pipe = pipeline(\"summarization\", model=\"../../../resources/embedding/google_pegasus-cnn-dm\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_125004/2994031358.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  bleu_metric = load_metric(\"sacrebleu\")\n",
      "Using the latest cached version of the module from /home/users/sunhongchao/.cache/huggingface/modules/datasets_modules/metrics/sacrebleu/31e1673407d8789b8f5ddfd979948f6a1de0a6d691426d55fa74a35ffb0c1bdf (last modified on Sat Oct 29 23:06:04 2022) since it couldn't be found locally at sacrebleu, or remotely on the Hugging Face Hub.\n",
      "Using the latest cached version of the module from /home/users/sunhongchao/.cache/huggingface/modules/datasets_modules/metrics/rouge/0ffdb60f436bdb8884d5e4d608d53dbe108e82dac4f494a66f80ef3f647c104f (last modified on Sat Oct 29 23:10:57 2022) since it couldn't be found locally at rouge, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric \n",
    "import pandas as pd \n",
    "bleu_metric = load_metric(\"sacrebleu\")\n",
    "rouge_metric = load_metric(\"rouge\") # rouge_score==0.0.4 work well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\" \n",
    "print(device)\n",
    "def chunks(list_of_elements, batch_size): \n",
    "\t\"\"\"Yield successive batch-sized chunks from list_of_elements.\"\"\" \n",
    "\tfor i in range(0, len(list_of_elements), batch_size): \n",
    "\t    yield list_of_elements[i : i + batch_size] \n",
    "def evaluate_summaries_pegasus(dataset, metric, model, tokenizer, batch_size=16, device=device, column_text=\"dialogue\", column_summary=\"summary\"): \n",
    "\tarticle_batches = list(chunks(dataset[column_text], batch_size)) \n",
    "\ttarget_batches = list(chunks(dataset[column_summary], batch_size)) \n",
    "\tfor article_batch, target_batch in tqdm( zip(article_batches, target_batches), total=len(article_batches)): \n",
    "\t\tinputs = tokenizer(article_batch, max_length=1024, truncation=True, padding=\"max_length\", return_tensors=\"pt\") \n",
    "\t\tsummaries = model.generate(input_ids=inputs[\"input_ids\"].to(device), attention_mask=inputs[\"attention_mask\"].to(device), length_penalty=0.8, num_beams=8, max_length=128) \n",
    "\t\tdecoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, clean_up_tokenization_spaces=True) for s in summaries] \n",
    "\t\tdecoded_summaries = [d.replace(\"<n>\", \" \") for d in decoded_summaries] \n",
    "\t\tmetric.add_batch(predictions=decoded_summaries, references=target_batch) \n",
    "\tscore = metric.compute() \n",
    "\treturn score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer \n",
    "\n",
    "model_ckpt = \"../../../resources/embedding/google_pegasus-cnn-dm\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at ../../../resources/dataset/hf_samsum/train/cache-c5296302575c0f7a.arrow\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Loading cached processed dataset at ../../../resources/dataset/hf_samsum/validation/cache-7bc2f4bf3670c069.arrow\n"
     ]
    }
   ],
   "source": [
    "def convert_examples_to_features(example_batch): \n",
    "\tinput_encodings = tokenizer(example_batch[\"dialogue\"], max_length=1024, truncation=True) \n",
    "\t# with tokenizer.as_target_tokenizer(): \n",
    "\ttarget_encodings = tokenizer(example_batch[\"summary\"], max_length=128, truncation=True) \n",
    "\treturn {\"input_ids\": input_encodings[\"input_ids\"], \"attention_mask\": input_encodings[\"attention_mask\"], \"labels\": target_encodings[\"input_ids\"]} \n",
    "dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched=True) \n",
    "columns = [\"input_ids\", \"labels\", \"attention_mask\"] \n",
    "dataset_samsum_pt.set_format(type=\"torch\", columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_API_KEY\"] = \"12fe58dbecee4ddfa653d6ac2ad2f09612ddecd0\"\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "# import wandb\n",
    "# wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq \n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model) \n",
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"] \n",
    "from transformers import TrainingArguments, Trainer \n",
    "training_args = TrainingArguments( output_dir='pegasus-samsum', num_train_epochs=1, warmup_steps=500, per_device_train_batch_size=1, per_device_eval_batch_size=1, weight_decay=0.01, logging_steps=10, push_to_hub=False,\n",
    "evaluation_strategy='steps', eval_steps=500, save_steps=1e6, gradient_accumulation_steps=16)\n",
    "trainer = Trainer(model=model, args=training_args, tokenizer=tokenizer, data_collator=seq2seq_data_collator, train_dataset=dataset_samsum_pt[\"train\"], eval_dataset=dataset_samsum_pt[\"validation\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `PegasusForConditionalGeneration.forward` and have been ignored: id, dialogue, summary. If id, dialogue, summary are not expected by `PegasusForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/home/users/sunhongchao/miniconda3/envs/qag_39/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 14732\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 230\n",
      "/home/users/sunhongchao/miniconda3/envs/qag_39/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='89' max='230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 89/230 24:58 < 40:29, 0.06 it/s, Epoch 0.38/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train() \n",
    "# score = evaluate_summaries_pegasus( dataset_samsum[\"test\"], rouge_metric, trainer.model, tokenizer, batch_size=2, column_text=\"dialogue\", column_summary=\"summary\") \n",
    "# rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names) \n",
    "# pd.DataFrame(rouge_dict, index=[f\"pegasus\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('qag_39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "744a839735711d1b6c70ee4177d5fcf08c66458e46a00b0e0b21f44edd608921"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
