import time

import os
import gradio as gr
from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.llms import ChatGLM
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA

# åŠ è½½embedding
embedding_model_dict = {
    "ernie-tiny": "nghuyong/ernie-3.0-nano-zh",
    "ernie-base": "nghuyong/ernie-3.0-base-zh",
    "text2vec": "GanymedeNil/text2vec-large-chinese",
    "text2vec2": "uer/sbert-base-chinese-nli",
    "text2vec3": "shibing624/text2vec-base-chinese",
}


#!/usr/bin/python3
# -*- encoding: utf-8 -*-
################################################################################
#
# Copyright (c) 2023 by Martain.AI, All Rights Reserved.
#
# Description: 
# Author: apollo2mars apollo2mars@gmail.com
################################################################################

import os
from langchain.document_loaders import DirectoryLoader, UnstructuredFileLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain.vectorstores import chroma, Chroma

embedding_mode_dict = {
    "ernie-tiny": "nghuyong/ernie-3.0-nano-tiny",
    # "text2vec3": "shibing624/text2vec-base-chinese"
    "text2vec3": "text2vec3"
}

print("begin")

def load_documents(directory='books'):
    # loader = DirectoryLoader('ä¸‰ä½“', glob="*.py", show_progress=True, use_multithreading=True)
    loader = TextLoader('ä¸‰ä½“.txt', encoding='utf-8')
    documents = loader.load()
    for document in documents:
        print(document)
    text_spliter = CharacterTextSplitter(chunk_size=64, 
                                         chunk_overlap=0)
    split_docs = text_spliter.split_documents(documents)
    return split_docs


def load_embedding_mode(model_name='shibing624/text2vec-base/chinese'):
    encode_kwargs = {"normalize_embeddings":False}
    model_kwargs = {"device": "cuda:0"}
    return HuggingFaceEmbeddings(model_name=embedding_mode_dict[model_name],
                                 model_kwargs=model_kwargs,
                                 encode_kwargs=encode_kwargs)

def store_chroma(docs, embeddings, persist_directory='VectorStore'):
    print(docs)
    print(embeddings)
    db = Chroma.from_documents(docs, embeddings, persist_directory)
    # db = Chroma
    db.persist()
    return db

documents = load_documents('books')
print("end load documents")

embeddings = load_embedding_mode('text2vec3')
print("end load embedding")

db = store_chroma(documents, embeddings)
print("end load db")

# if not os.path.exists("VectorStore"):
#     documents = load_documents()
#     print("end load documents")
#     db = store_chroma(documents, embeddings)
#     print("end load db")
# else:
#     db = chroma(persisi_distory="VectorStore", embedding_function=embeddings)


# def load_documents(directory="ä¸‰ä½“"):
#     """
#     åŠ è½½booksä¸‹çš„æ–‡ä»¶ï¼Œè¿›è¡Œæ‹†åˆ†
#     :param directory:
#     :return:
#     """
#     loader = DirectoryLoader(directory)
#     documents = loader.load()
#     text_spliter = CharacterTextSplitter(chunk_size=256, chunk_overlap=0)
#     split_docs = text_spliter.split_documents(documents)
#     return split_docs

def load_documents(directory='books'):
    # loader = DirectoryLoader('ä¸‰ä½“', glob="*.py", show_progress=True, use_multithreading=True)
    loader = TextLoader('./ä¸‰ä½“/ä¸‰ä½“1.txt', encoding='utf-8')
    documents = loader.load()
    for document in documents:
        print(document)
    text_spliter = CharacterTextSplitter(chunk_size=32, 
                                         chunk_overlap=0)
    split_docs = text_spliter.split_documents(documents)
    return split_docs


def load_embedding_model(model_name="ernie-tiny"):
    """
    åŠ è½½embeddingæ¨¡å‹
    :param model_name:
    :return:
    """
    encode_kwargs = {"normalize_embeddings": False}
    model_kwargs = {"device": "cuda:0"}
    return HuggingFaceEmbeddings(
        model_name=embedding_model_dict[model_name],
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )


def store_chroma(docs, embeddings, persist_directory="VectorStore"):
    """
    è®²æ–‡æ¡£å‘é‡åŒ–ï¼Œå­˜å…¥å‘é‡æ•°æ®åº“
    :param docs:
    :param embeddings:
    :param persist_directory:
    :return:
    """
    db = Chroma.from_documents(docs, embeddings, persist_directory=persist_directory)
    db.persist()
    return db


# åŠ è½½embeddingæ¨¡å‹
embeddings = load_embedding_model('text2vec3')
# åŠ è½½æ•°æ®åº“
if not os.path.exists('VectorStore'):
    print(">>>")
    documents = load_documents()
    db = store_chroma(documents, embeddings)
else:
    db = Chroma(persist_directory='VectorStore', embedding_function=embeddings)
# åˆ›å»ºllm
llm = ChatGLM(
    endpoint_url='http://127.0.0.1:8000',
    max_token=16,
    top_p=0.9
)
# åˆ›å»ºqa
retriever = db.as_retriever()
qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type='stuff',
    retriever=retriever,
    verbose=True
)


response = qa.run('å¶æ–‡æ´çš„å¦¹å¦¹æ˜¯è°')


# def add_text(history, text):
#     history = history + [(text, None)]
#     return history, gr.update(value="", interactive=False)


# def add_file(history, file):
#     """
#     ä¸Šä¼ æ–‡ä»¶åçš„å›è°ƒå‡½æ•°ï¼Œå°†ä¸Šä¼ çš„æ–‡ä»¶å‘é‡åŒ–å­˜å…¥æ•°æ®åº“
#     :param history:
#     :param file:
#     :return:
#     """
#     global qa
#     directory = os.path.dirname(file.name)
#     documents = load_documents(directory)
#     db = store_chroma(documents, embeddings)
#     retriever = db.as_retriever()
#     qa.retriever = retriever
#     history = history + [((file.name,), None)]
#     return history


# def bot(history):
#     """
#     èŠå¤©è°ƒç”¨çš„å‡½æ•°
#     :param history:
#     :return:
#     """
#     message = history[-1][0]
#     if isinstance(message, tuple):
#         response = "æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼ï¼"
#     else:
#         response = qa.run(message)
#     history[-1][1] = ""
#     for character in response:
#         history[-1][1] += character
#         time.sleep(0.05)
#         yield history


# with gr.Blocks() as demo:
#     chatbot = gr.Chatbot(
#         [],
#         elem_id="chatbot",
#         bubble_full_width=False,
#         avatar_images=(None, (os.path.join(os.path.dirname(__file__), "avatar.png"))),
#     )

#     with gr.Row():
#         txt = gr.Textbox(
#             scale=4,
#             show_label=False,
#             placeholder="Enter text and press enter, or upload an image",
#             container=False,
#         )
#         btn = gr.UploadButton("ğŸ“", file_types=['txt'])

#     txt_msg = txt.submit(add_text, [chatbot, txt], [chatbot, txt], queue=False).then(
#         bot, chatbot, chatbot
#     )
#     txt_msg.then(lambda: gr.update(interactive=True), None, [txt], queue=False)
#     file_msg = btn.upload(add_file, [chatbot, btn], [chatbot], queue=False).then(
#         bot, chatbot, chatbot
#     )

# demo.queue()
# if __name__ == "__main__":
#     demo.launch(share=True)